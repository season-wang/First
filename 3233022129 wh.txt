import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体
matplotlib.rcParams['axes.unicode_minus'] = False   # 正常显示负号

# 1. 读取数据
print("正在读取数据...")
try:
    # 假设train.csv有两列：x和y
    data = pd.read_csv('train.csv')
    print("数据读取成功！")
    print(f"数据形状: {data.shape}")
    print(data.head())
except FileNotFoundError:
    print("未找到train.csv文件，生成示例数据...")
    # 生成示例数据
    np.random.seed(42)
    x = np.linspace(0, 10, 100)
    y = 2.5 * x + 1.2 + np.random.normal(0, 1, 100)
    data = pd.DataFrame({'x': x, 'y': y})
    data.to_csv('train.csv', index=False)
    print("已生成示例数据并保存为train.csv")

# 准备数据
X = data[['x']].values if 'x' in data.columns else data.iloc[:, 0].values.reshape(-1, 1)
y = data['y'].values if 'y' in data.columns else data.iloc[:, 1].values

# 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# 2. 线性回归模型训练
class LinearRegression:
    def __init__(self):
        self.w = 0
        self.b = 0
        self.loss_history = []
        
    def predict(self, X):
        return self.w * X + self.b
    
    def compute_loss(self, X, y):
        y_pred = self.predict(X)
        return np.mean((y_pred - y) ** 2)
    
    def fit(self, X, y, learning_rate=0.01, epochs=1000):
        n = len(X)
        
        for epoch in range(epochs):
            # 前向传播
            y_pred = self.predict(X)
            
            # 计算损失
            loss = np.mean((y_pred - y) ** 2)
            self.loss_history.append(loss)
            
            # 反向传播 - 计算梯度
            dw = (2/n) * np.sum(X * (y_pred - y))
            db = (2/n) * np.sum(y_pred - y)
            
            # 更新参数
            self.w -= learning_rate * dw
            self.b -= learning_rate * db
            
            if epoch % 100 == 0:
                print(f'Epoch {epoch}, Loss: {loss:.4f}')
        
        print(f'训练完成! 最终参数: w={self.w:.4f}, b={self.b:.4f}')
        return self.w, self.b, self.loss_history

# 训练模型
print("\n开始训练模型...")
model = LinearRegression()
w_final, b_final, loss_history = model.fit(X_train.flatten(), y_train, learning_rate=0.1, epochs=1000)

# 3. 绘制w和loss之间的关系、b和loss之间的关系
print("\n生成可视化图表...")

# 创建网格来探索参数空间
w_range = np.linspace(w_final-3, w_final+3, 50)
b_range = np.linspace(b_final-3, b_final+3, 50)

# 计算不同w值下的loss（固定b为最优值）
w_losses = []
for w in w_range:
    temp_model = LinearRegression()
    temp_model.w = w
    temp_model.b = b_final
    loss = temp_model.compute_loss(X_test.flatten(), y_test)
    w_losses.append(loss)

# 计算不同b值下的loss（固定w为最优值）
b_losses = []
for b in b_range:
    temp_model = LinearRegression()
    temp_model.w = w_final
    temp_model.b = b
    loss = temp_model.compute_loss(X_test.flatten(), y_test)
    b_losses.append(b)

# 创建图表
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

# 图1: w和loss的关系
ax1.plot(w_range, w_losses, 'b-', linewidth=2)
ax1.axvline(x=w_final, color='r', linestyle='--', alpha=0.7, label=f'最优 w = {w_final:.4f}')
ax1.set_xlabel('权重 w')
ax1.set_ylabel('损失 Loss')
ax1.set_title('权重 w 与损失 Loss 的关系')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 图2: b和loss的关系
ax2.plot(b_range, b_losses, 'g-', linewidth=2)
ax2.axvline(x=b_final, color='r', linestyle='--', alpha=0.7, label=f'最优 b = {b_final:.4f}')
ax2.set_xlabel('偏置 b')
ax2.set_ylabel('损失 Loss')
ax2.set_title('偏置 b 与损失 Loss 的关系')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 图3: 训练过程中的损失下降
ax3.plot(loss_history, 'r-', linewidth=2)
ax3.set_xlabel('训练轮次 Epoch')
ax3.set_ylabel('损失 Loss')
ax3.set_title('训练过程中损失的变化')
ax3.set_yscale('log')  # 使用对数坐标更好地观察损失下降
ax3.grid(True, alpha=0.3)

# 图4: 最终拟合结果
ax4.scatter(X_test, y_test, alpha=0.7, label='测试数据')
x_line = np.linspace(X_test.min(), X_test.max(), 100)
y_line = model.predict(x_line)
ax4.plot(x_line, y_line, 'r-', linewidth=2, label=f'拟合直线: y = {w_final:.2f}x + {b_final:.2f}')
ax4.set_xlabel('特征 x')
ax4.set_ylabel('目标 y')
ax4.set_title('模型拟合结果')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('linear_regression_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# 输出最终结果
print(f"\n=== 模型训练结果 ===")
print(f"最终权重 w: {w_final:.4f}")
print(f"最终偏置 b: {b_final:.4f}")
print(f"最终损失: {loss_history[-1]:.4f}")

# 测试集性能
test_predictions = model.predict(X_test.flatten())
test_loss = np.mean((test_predictions - y_test) ** 2)
print(f"测试集损失: {test_loss:.4f}")

# 保存模型参数
results = pd.DataFrame({
    'parameter': ['w', 'b', 'final_loss', 'test_loss'],
    'value': [w_final, b_final, loss_history[-1], test_loss]
})
results.to_csv('model_parameters.csv', index=False)
print("模型参数已保存到 model_parameters.csv")